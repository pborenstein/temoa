---
type: daily
title: Normcore LLM Reads
tags:
---

|<< [[Daily/2025/09-September/2025-09-04-Th\|Yesterday]] \| [[Daily/2025/09-September/2025-09-06-Sa\|Tomorrow]] >>|

## 2025-09-05 

- Skipped the workout. Kind of cranky I was at 5am
- Kline
- [[Robert Sapolsky]] free will etc via Mark Kline


## File activity

> [!Notes created today]-
>```dataview
>List FROM "" WHERE file.cday = date("2025-09-05") and !contains(file.tags, "#daily") SORT file.ctime asc
>```

### Notes created today

<!-- QueryToSerialize: List FROM "" WHERE file.cday = date("2025-09-05") and !contains(file.tags, "#daily") SORT file.ctime asc -->



> [!Notes modified today]-
> recent first
>```dataview
>List FROM "" WHERE file.mday = date("2025-09-05") and !contains(file.tags, "#daily") SORT file.mtime desc
>```


### Notes modified today

<!-- QueryToSerialize: List FROM "" WHERE file.mday = date("2025-09-05") and !contains(file.tags, "#daily") SORT file.mtime desc -->


## Gleanings

> what did we surf into now?

- [Transformer Explainer: LLM Transformer Model Visually Explained](https://poloclub.github.io/transformer-explainer/)  [12:08]
> What is a Transformer?
> - Transformers are neural network architectures changing AI approaches.  
> - First introduced in the paper "Attention is All You Need" in 2017.  
> - Used in text-generative models like GPT, Llama, and Gemini.  
> - Operate on next-word prediction principle using self-attention.  
> - Comprise key components: Embedding, Transformer Block, and Output Probabilities.  
> - Embedding converts tokens into numerical representations.  
> - Transformer Blocks contain multi-head self-attention and MLP layers.  
> - Final outputs are probabilities for the next token.

- [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)  [18:35]
> Deep Dive into LLMs like ChatGPT
> - Deep dive into Large Language Model (LLM) AI technology.
> - Covers training stack of models like ChatGPT.
> - Discusses practical applications and psychology of LLMs.
> - Features chapters on topics like tokenization, neural networks, and reinforcement learning.
> - Presented by Andrej Karpathy, an expert in AI and founding member of OpenAI.
> - Video duration is 3 hours and 31 minutes with over 3.5 million views.

- [From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)  [18:36]
> From GPT-2 to gpt-oss: Analyzing the Architectural Advances
> - OpenAI released gpt-oss models, first open-weight models since GPT-2.
> - Models are optimized for local execution with MXFP4.
> - Article compares gpt-oss architecture to GPT-2 and Qwen3.
> - Key features include Mixture-of-Experts, grouped query attention, and RMSNorm.
> - Models trained for reasoning tasks with adjustable reasoning effort.
> - Benchmarks show competitive performance against proprietary models like GPT-5.

- [karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs.](https://github.com/karpathy/nanoGPT)  [18:44]
> karpathy/nanoGPT
> - The repository is for training/finetuning medium-sized GPTs.
> - It is a simpler version of minGPT.
> - Users can train a character-level GPT on Shakespeare's works quickly.
> - Code is beginner-friendly, making modifications easy.
> - It allows finetuning of pretrained models like GPT-2.
> - Supports Python and various libraries such as PyTorch, NumPy, and transformers.

- [Transformer Explainer: LLM Transformer Model Visually Explained](https://poloclub.github.io/transformer-explainer/)  [18:45]
> What is a Transformer?
> - Transformers are a neural network architecture introduced in 2017.
> - They revolutionized AI, powering models like GPT and Gemini.
> - Key components include embedding, transformer blocks, and output probabilities.
> - Self-attention allows for capturing long-range dependencies.
> - The process begins with tokenization, followed by embedding, positional encoding, and through multiple transformer blocks to generate predictions.
> - Advanced features like layer normalization, dropout, and residual connections enhance model performance.
> - Interactive features allow users to visualize and manipulate transformer functionalities.

- [Normcore LLM Reads](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)  [19:07]
> Anti-hype LLM reading list
> - Goals: Add reasonable links explaining how language models (LLMs) work, avoiding hype and vendor content.
> - Includes foundational concepts, building blocks, and significant papers on LLMs.
> - Covers various topics like GPT, training data, deployment, and evaluation.
> - Provides external resources for prompt engineering, GPU recommendations, and UX considerations.
